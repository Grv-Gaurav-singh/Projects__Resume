The project focuses on revolutionizing navigation for individuals with visual 
impairments by leveraging cutting-edge technologies. It employs a vision 
sensor to capture the surrounding visual environment, and this data is 
processed using OpenCV, creating a framework that divides the space into 
distinct segments. Through the implementation of a neural network for object 
detection, the system provides real-time feedback, categorizing the visual 
segments as obstacle-free or obstructed. The output signals, including "Move 
Left," "Move Right," or "Stop," are generated based on this analysis, facilitating 
immediate and intuitive guidance for the user. Haptic technology comes into 
play through linear resonant actuators integrated into wearable shoes. These 
actuators are activated based on the received signals, offering tactile feedback 
to the user, enhancing their spatial awareness and aiding in navigation. This 
interdisciplinary approach, combining computer vision, machine learning, and 
haptic feedback, showcases an innovative solution to empower visually 
impaired individuals in navigating their surroundings more confidently and 
naturally.
